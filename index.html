<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AWS RDB to EMAIL</title>
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@400;700&display=swap" rel="stylesheet">
    <link href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/themes/prism-tomorrow.min.css" rel="stylesheet" />
    <style>
        body { font-family: 'Roboto', sans-serif; line-height: 1.6; padding: 20px 50px; font-size: 18px;}
        .header, .footer { background: linear-gradient(to right, #8145d1 0%, #cc8245 100%); padding: 10px 20px; text-align: center; color: white; }
        .content { margin: 20px auto; max-width: 1200px; }
        .section { margin-bottom: 20px; }
        h1, h2, h3 { font-weight: 700; }
        img, iframe { max-width: 100%; height: auto; }
        nav { text-align: center; margin-top: 10px; }
        nav a { margin: 0 10px; text-decoration: none; color: inherit; font-weight: bold; }
        .top-left-image { position: absolute; top: 45px; left: 100px; border-radius: 5px; }
        .image-container {display: flex;align-items: flex-start;}
        .left-image {margin-right: 10px; /* Adjust the margin as needed */}
        .section-divider {border: 0;height: 10px;background-color: orange; /* Adjust the color as needed */margin-top: 20px; /* Adjust the spacing as needed */
    margin-bottom: 40px; /* Adjust the spacing as needed */}


    </style>
</head>
<body>
    <div class="top-left-image">
        <img src="images/projecticon.png" alt="Top Left Image">
    </div>
    <div class="header">
        <h1>Daily graph from RDBMS to Email using AWS Services</h1>
        <nav>
            <a href="#about">About</a>
            <a href="#step1">Step 1 - Creating DB on Heroku</a>
            <a href="#step2">Step 2 - RDBMS to S3 using Lambda</a>
            <a href="#step3">Step 3 - Drawing graphs on EC2</a>
            <a href="#step4divider">Step 4 - Email the graph</a>
            <a href="#step5divider">Screenshots from AWS</a>
        </nav>
    </div>

    <div class="content">
        <div id="about" class="section">
            <h2>About This Project</h2>
            <p>I developed this project to make it easier for the finance department to get daily sales graphs. It's designed to be flexible, handling various data requests seamlessly. Every day, the team accesses up-to-date reports and the associated CSVs and images stored on S3.</p>
            <p>To achieve this, I utilized six AWS services—Lambda, EC2, S3, SES, EventBridge, and IAM for role management—along with Heroku PostgreSQL. Opting for an external database with Heroku PostgreSQL alongside AWS allowed us to enhance the flexibility of our data management setup.</p>
            <p>Initially, I started with Lambda but soon encountered limitations related to size and compatibility issues with matplotlib. Switching to an EC2 instance solved these problems and also provided a scalable solution for future tasks. Now, every weekday morning, each department member receives an email with the latest graph. This simple yet effective change has streamlined our decision-making process, keeping everyone informed and aligned.</p>
            <img src="images/v01.drawio.png" alt="Organizational chart of the services showing all tables and relationships" style="max-width:100%; height:auto; background-color:#333;">
            <p>This diagram shows the structure of my project with all the AWS services and their relationships. It’s a handy reference for understanding how my services is organized and interconnected.</p>
        </div>
        <hr class="section-divider">
        <div id="step1" class="section">
            <h2>Step 1 - Creating DB on Heroku</h2>
            <p>For this project, I decided to use the Microsoft Adventure Works database. It’s originally built for MS SQL Server and is perfect for showing off what a real e-commerce operation might look like. I had to shift this over to Heroku PostgreSQL, which was a bit of a challenge, but using pgAdmin helped me get all 29 tables set up using CSV files.</p>
            <p>The database is really the backbone of our application. It has everything from inventory management to customer interactions, and each table has its own set of rules to make sure the data stays clean and useful. I went with Adventure Works because it's complex enough to really demonstrate what you can do with advanced SQL and proper database management in a real-world business context.</p>
            <p>Let's start with a code that is used for a table. This is used to create a table and all the column properties are mentioned</p>
            
            <pre><code class="language-sql">
CREATE TABLE DimReseller (
    ResellerKey SERIAL PRIMARY KEY,
    GeographyKey INT,
    ResellerAlternateKey VARCHAR(15),
    Phone VARCHAR(25),
    BusinessType VARCHAR(20) NOT NULL,
    ResellerName VARCHAR(50) NOT NULL,
    NumberEmployees INT,
    MinPaymentAmount MONEY,
    #add other 12 columns
);
            </code></pre>
            <p>Here I had to do some modifications because CSV files already had primary key. So i chnaged ResellerKey to
                ResellerKey INT PRIMARY KEY before I uploaded CSV to the table.</p>
            <p><a href="https://github.com/jkehler/awslambda-psycopg2">The code for creating all tables is here</a></p>
            <p>There are 29 tables in the DB so I had to do this for all 29 of them. At the end I had all of them in the DB</p>
            <img src="images/29table.png" alt="Descriptive Alt Text">
            <p>Along the way I had to do some more altercations because the CSV files had some mismatches with the tables I 
                created.</p>
            <pre><code class="language-sql">
ALTER TABLE newfactcurrencyrate
ALTER COLUMN currencykey TYPE FLOAT;
            </code></pre>
            <p>All table creation codes can be found in the repo.</p>
        </div>
        <hr class="section-divider">
        <div id="step2" class="section">
            <h2>Step 2 - RDBMS to S3 using Lambda</h2>
            <p>The first part of the code was to extract data from a Postgres db by running a query and saving results as csv. This task is going to be repeated everyday. I will explain how that part of the task was achieved later in the project.  </p>
            <p>When I tried to use site packages of psycopg2 I had issues until I found this helpful github repo on <a href="https://github.com/jkehler/awslambda-psycopg2">jkehler/awslambda-psycopg2</a> . They solved the issue by compiling psycopg2 with the PostgreSQL libpq.so library instead of the default dynamic link.</p> 
            <h3>Set up environment for function:</h3>
            <p>I used the <b>psycopg2-3.11</b> version and created a lambda function with the same configuration. 
                I also used the <b>AWSSDKPandas-Python311</b> layer. You should <u>create a layer</u> to access the pandas library. </p>
            <h3>Code:</h3>
            <p>Here is important parts of the the code for the lambda function I used. You can get the entire code here:</p>
            <p><a href="https://github.com/omerbaktir/dailygraphtoemail/blob/main/code/lambda_for_retrieving_data.py">Code for Lambda</a></p>
            <pre><code class="language-python">
# import boto3,json,pandas,psycopg2,io,datetime and os 
# define lambda handler with DB parameters
# set up the connection
# execute the query
        cursor.execute("SELECT city, sum(yearlyincome) as avg_income FROM prospectivebuyer group by city order by avg_income desc limit 10;")
# fetch result and convert to a DataFrame and save as csv
# Create an S3 client
        s3_client = boto3.client('s3')
        today_date = datetime.now().strftime('%Y-%m-%d')
        
#Here I used datetime function to get date later on to use as a filename. 
# Upload CSV to S3
        bucket_name = 'yourbucket'
        object_name = f'graphs/sales_daily_{today_date}.csv'
#Since this process is going to be repeated I want an automated name for eveyday's file.
# Close the cursor and connection
# Return the result as JSON
        </code></pre>
        <img src="images/iam.jpg" alt="Descriptive Alt Text" style="display: block; margin-left: auto; margin-right: auto;">
        <h3>Creating a role for Lambda:</h3>
        <p>Last thing but not the least about AWS lambda is that we need to create a role from IAM and 
            attach this to the function. This function should be able to write to S3.
        <p>
            Lambda function needs a role to be able to write to S3 bucket. So I created an S3 access role in IAM (Identity Access Management) and allowed full access to the role. Then I attached this role to the function from Function > Configuration > General Configuration > Existing Role.
            
            At this point Lambda can read from the RDBMS and saves it to the defined bucket. Next step is to schedule this.
        </p>
        <p>I also increased the timeout to 10 secs and this can be adjusted according to the time code will need to run in order not to timeout.</p>
        </p>
        <img src="images/eventbridge.jpg" alt="Descriptive Alt Text" style="display: block; margin-left: auto; margin-right: auto;">
        <h3>Scheduling Lambda:</h3>
        <p>This is time to schedule the task. We need to use Event bridge and create schedule for this function.
            I cretaed a schedule for this function to run everyday at 5 pm (this can be adjusted) to retrieve
            the data and save it to S3.
        </p>
        </div>
        <hr class="section-divider">
        <div id="step3" class="section">
            <h2>Step 3 - Drawing graphs on EC2</h2>
            <h3>Initial plan and adjustment:</h3>
            <p>I was planning to use Lambda throuhout but Lambda could not handle matplotlib the way I expected.
                When I tried using layers that I cretaed it was too large (larger than 250 Mb unzipped). This was
                more than AWS allows even using S3 to store the layer. When I deleted some libraries pandas didn't
                function well. I also tried to create libraries using manylinux wheels but I could not solve dependency 
                issues. So I revised my plan. I decided to use EC2 and schedule EC2 to save costs.
            </p>
            <h3>Setting up EC2 and testing code locally (in EC2):</h3>
            <p>I created an instance with minimal configurations. This task is not compute heavy and this instance can 
                be used for other automation tasks. I used Amazon Linux. After creating the instance I tried the code locally.
                I was able to save file locally.
            </p>
            <h3>Connection with S3:</h3>
            <p>After testing locally I connected to S3 to read and write. As a part of this project we had to save files
                on S3 to keep as further processing. All CSV files are stored by Lambda function and all images 
                after drawing the graph are stored by EC2.
            </p>
            <p>To connect S3 we again need to use <b>IAM roles</b> and attach a role to EC2. This role has permission
            to read and write permissions from S3</p>
            <h3>Code for creating a graph using Pandas and Matplotlib:</h3>
            <p>Here are some insights from code. Complete code can be found at: </p>
            <p><a href="https://github.com/omerbaktir/dailygraphtoemail/blob/main/code/draw_graph.py">Code for Drawing graph</a></p>
            <pre><code class ="language-python" >
# import pandas,matplotlib,boto3,BytesIO from io and datetime
# Initialize a session on S3
s3 = boto3.client('s3')
# Today's date for dynamic file access and saving
today = datetime.now().strftime("%Y-%m-%d")
# Bucket and object key for reading the CSV dynamically based on today's date
bucket_name = 'yourbucket'
object_key = f'graphs/sales_daily_{today}.csv'
# Get the object from the bucket
obj = s3.get_object(Bucket=bucket_name, Key=object_key)
# Read data from the object's body directly into a DataFrame
data = pd.read_csv(BytesIO(obj['Body'].read()))
# Plotting the bar graph and formatting
# Format y-axis to display values in a readable format
plt.gca().get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: "{:,}".format(int(x))))
# Save the plot to a BytesIO object
img_data = BytesIO()
plt.savefig(img_data, format='png')
img_data.seek(0)  # Rewind the BytesIO object
# Upload the image to S3
upload_bucket = 'uploadbucket'
upload_key = f'plots/daily_graph_{today}.png'  # You can change the directory structure if needed
s3.put_object(Bucket=upload_bucket, Key=upload_key, Body=img_data, ContentType='image/png')
            </code></pre>
            <h3>Scheduling the task using crontab:</h3>
            <p>I had to schedule this task using crontab. EC2 is going to be active certain times of the day
                to save costs and cron will automate the job. Here is the cron I used:
            </p>
            <img src="images/crontab.png" alt="Descriptive Alt Text">
            <p>This code means this task will be repeated everyday at 21:06 and one more detail: If you do not set up
                <b>Amazon Linux uses UTC.</b> So I had to adjust the timing according to that.</p>
            <p>Now the graph is stored in S3 with a unique name for the day.</p>
            <h3>Managing EC2 with scheduled start and stop times:</h3>
            <p>EC2 can run whole day if necessary but we pay as much hours as we use. So, I decided to schedule this.
                We can again use <b>Lambda with Event Bridge</b> to schedule this so we don't use EC2 unnecessarily.</p>
            <p>Lambda code for starting and stopping EC2:</p>
            <pre><code class = "language-python">
import boto3
def lambda_handler(event, context):
    ec2 = boto3.client('ec2', region_name='your-region')  # Specify your region
    instances = ['instance']  # Specify your instance ID
    ec2.start_instances(InstanceIds=instances)
    return {
        'message': "EC2 instance started"
    }
            </code></pre>
            <pre><code class = "language-python">
import boto3
def lambda_handler(event, context):
    ec2 = boto3.client('ec2', region_name='your-region')  # Specify your region
    instances = ['instance-id']  # Specify your instance ID
    ec2.stop_instances(InstanceIds=instances)
    return {
        'message': "EC2 instance stopped"
    }
                            </code></pre>
        <h3>Scheduling Lambda functions to start and stop EC2:</h3>
        <p>After creating Lambda fucntions I scheduled them to start and stop using Event Bridge. I 
            planned 45 minutes for EC2 to run but this time can be adjusted according to needs of company
            and also for running multiple tasks on EC2. For high end instances it is important to manage run times.
        </p>
        </div>
        <hr id ="step4divider" class="section-divider">
        <img id = "image4" src="images/Amazon-SES.png" alt="Descriptive Alt Text" style="display: block; margin-left: auto; margin-right: auto;">
        <div id="step4" class="section">
            <h2>Step 4 - Email the graph</h2>
            <h3>Setting up SES:</h3>
            <p>This AWS Service needs more set up than other services. You need the register your email and also
                a domain if possible. I have my own domain so I registered both. For email you get a confirmation
                email and it is done. For domain you need to add keys to your DNS on domain providers site. After that
                are still in sandbox and you need to request for AWS to take you out of sandbox.
            </p>
            <p>You can only send to registered emails in sandbox with a limit of 200 emails/day and 1 email/second.</p>
            <p>I requested for that and it took around a day to get approved.</p>
            <h3>Setting up Lambda function to send emails:</h3>
            <p>We need another function to send emails to the team. Whenever EC2 is done with the graph and image is stored
                in S3. S3 triggres this lambda function and Lambda sends an email with a predefined text to team members
                with the graph attached. This lambda fuction also needs permission for S3 and SES. These can be cretaed 
                in IAM and attached to the function.
            </p>
            <p>Here is the fuction to send emails. Complete code can be found at:</p>
            <p><a href="https://github.com/omerbaktir/dailygraphtoemail/blob/main/code/lambda_for_sendin_email.py">Code for sending emails</a></p>
            <pre><code class = "language-python">
#import boto3,base64,os,...
#define lambda_handler with s3 and ses clients
# Extract bucket name and key from the event
# Get the file content from S3
# Create a MIME multipart message (use a txt file to store email addresses in the directory with fuction)
    with open('emails.txt', 'r') as file:
        email_list = [line.strip() for line in file if line.strip()]
    msg = MIMEMultipart()
    msg['Subject'] = 'Daily Sales Graph'
    msg['From'] = 'your@email.net'
    msg['To'] = ', '.join(email_list)
    # Attach text body
    msg.attach(MIMEText('Here is the daily sales graph.', 'plain'))
    # Attach the image
    img = MIMEImage(file_content)
    img.add_header('Content-Disposition', 'attachment', filename=key)
    msg.attach(img)
# Send the email
    </code>
            </pre>
        </div>
        <hr id ="step5divider" class="section-divider">
        <div id="screenshots" class="section">
            <h2>Screenshots</h2>
            <h3>Database overall</h3>
            <p>If you have any questions, please feel free to reach out to me at <a href="mailto:your.email@example.com">your.email@example.com</a>.</p>
                   
        </div>
    </div>

    <div class="footer">
        <p>Copyright &copy; 2024 baktir.net</p>
    </div>
    <!-- Include Prism.js JavaScript -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/prism.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/components/prism-python.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/prism/1.25.0/components/prism-sql.min.js"></script>
</body>
</html>
